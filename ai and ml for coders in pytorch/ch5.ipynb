{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b916d45",
   "metadata": {},
   "source": [
    "### Chapter 5. Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b11833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary Index: {'today': 1, 'is': 2, 'a': 3, 'sunny': 4, 'day': 5, 'rainy': 6}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sentences = [\n",
    "    'Today is a sunny day',\n",
    "    'Today is a rainy day'\n",
    "]\n",
    "# Tokenization function\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build the vocabulary\n",
    "def build_vocab(sentences):\n",
    "    vocab={}\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)+1\n",
    "    return vocab\n",
    "\n",
    "# create the vocabulary index\n",
    "vocab = build_vocab(sentences)\n",
    "print(\"vocabulary Index:\",vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab34b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b18a9709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [['[CLS]', 'today', 'is', 'a', 'sunny', 'day', '[SEP]'], ['[CLS]', 'today', 'is', 'a', 'rainy', 'day', '[SEP]']]\n",
      "Token IDs: tensor([[  101,  2651,  2003,  1037, 11559,  2154,   102],\n",
      "        [  101,  2651,  2003,  1037, 16373,  2154,   102]])\n",
      "Word Index: {'##eb': 15878, 'nearest': 7205, 'prequel': 28280, 'unauthorized': 24641, '##ʿ': 29714, 'vegetation': 10072, '##nsen': 29428, 'malabar': 28785, 'terra': 14403, 'ョ': 1730}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "# intialize the tokenizer \n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "# Tokenize the sentences and encode them\n",
    "encoded_inputs = tokenizer(sentences,padding=True,truncation=True,return_tensors='pt')\n",
    "# To see the tokens for each input (helpful for understanding the output)\n",
    "tokens = [tokenizer.convert_ids_to_tokens(ids) \n",
    "           for ids in encoded_inputs[\"input_ids\"]]\n",
    " \n",
    "# To get the word index similar to Keras' tokenizer\n",
    "word_index = tokenizer.get_vocab()\n",
    " \n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", encoded_inputs['input_ids'])\n",
    "print(\"Word Index:\", dict(list(word_index.items())[:10]))  \n",
    "# show only the first 10 for brevity\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b2fd6",
   "metadata": {},
   "source": [
    "BERT (which stands for bidirectional encoder representations from transformers)\n",
    "\n",
    "\n",
    "Now, you may be wondering what [CLS] and [SEP] are—and how the BERT model has been trained to expect sentences to begin with [CLS] (for classifier) and end with or be separated by [SEP] (for separator). These two expressions are tokenized to values 101 and 102, respectively, so when you print out the token values for your sentences.\n",
    "\n",
    "Either way, once you have the words in your sentences tokenized, the next step is to convert your sentences into lists of numbers, with the number being the value where the word is the key. This process is called sequencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8126fe8",
   "metadata": {},
   "source": [
    "### Stripping HTML Tags\n",
    "Use the BeautifulSoup library to remove HTML tags. Here's an example:\n",
    "\n",
    "```\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(sentence)\n",
    "sentence = soup.get_text()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5dfc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3016b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
