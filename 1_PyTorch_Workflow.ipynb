{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcKSD0nYGnIB",
        "outputId": "41c77660-3518-4fa6-b85e-6bc0e86a2fb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{1: 'data (prepare and load)',\n",
              " 2: 'build model',\n",
              " 3: 'fitting the model to data (training)',\n",
              " 4: 'making predictions and evaluting a model (inference)',\n",
              " 5: 'saving and loading a model',\n",
              " 6: 'putting it all together'}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pytorch workflow\n",
        "what_were_covering = {1: \"data (prepare and load)\",\n",
        "                      2: \"build model\",\n",
        "                      3: \"fitting the model to data (training)\",\n",
        "                      4: \"making predictions and evaluting a model (inference)\",\n",
        "                      5: \"saving and loading a model\",\n",
        "                      6: \"putting it all together\"}\n",
        "\n",
        "what_were_covering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.7.0-cp313-cp313-win_amd64.whl.metadata (29 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch)\n",
            "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting setuptools (from torch)\n",
            "  Downloading setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Downloading MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
            "Downloading torch-2.7.0-cp313-cp313-win_amd64.whl (212.5 MB)\n",
            "   ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.3/212.5 MB 10.7 MB/s eta 0:00:20\n",
            "   - -------------------------------------- 6.6/212.5 MB 18.8 MB/s eta 0:00:11\n",
            "   -- ------------------------------------- 11.3/212.5 MB 20.1 MB/s eta 0:00:11\n",
            "   --- ------------------------------------ 16.5/212.5 MB 21.5 MB/s eta 0:00:10\n",
            "   --- ------------------------------------ 21.2/212.5 MB 22.0 MB/s eta 0:00:09\n",
            "   ----- ---------------------------------- 28.8/212.5 MB 24.7 MB/s eta 0:00:08\n",
            "   ------ --------------------------------- 36.7/212.5 MB 26.7 MB/s eta 0:00:07\n",
            "   -------- ------------------------------- 45.1/212.5 MB 28.4 MB/s eta 0:00:06\n",
            "   ---------- ----------------------------- 53.5/212.5 MB 29.8 MB/s eta 0:00:06\n",
            "   ----------- ---------------------------- 61.9/212.5 MB 31.0 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 70.3/212.5 MB 31.9 MB/s eta 0:00:05\n",
            "   -------------- ------------------------- 78.4/212.5 MB 32.7 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 86.8/212.5 MB 33.4 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 94.4/212.5 MB 33.5 MB/s eta 0:00:04\n",
            "   ------------------ -------------------- 102.0/212.5 MB 33.7 MB/s eta 0:00:04\n",
            "   ------------------- ------------------- 108.3/212.5 MB 33.6 MB/s eta 0:00:04\n",
            "   -------------------- ------------------ 113.0/212.5 MB 32.8 MB/s eta 0:00:04\n",
            "   ---------------------- ---------------- 121.1/212.5 MB 33.2 MB/s eta 0:00:03\n",
            "   ----------------------- --------------- 129.5/212.5 MB 33.6 MB/s eta 0:00:03\n",
            "   ------------------------- ------------- 138.4/212.5 MB 34.0 MB/s eta 0:00:03\n",
            "   -------------------------- ------------ 145.8/212.5 MB 34.2 MB/s eta 0:00:02\n",
            "   --------------------------- ----------- 151.5/212.5 MB 33.9 MB/s eta 0:00:02\n",
            "   ----------------------------- --------- 158.3/212.5 MB 33.9 MB/s eta 0:00:02\n",
            "   ------------------------------ -------- 163.8/212.5 MB 33.5 MB/s eta 0:00:02\n",
            "   ------------------------------- ------- 171.7/212.5 MB 33.6 MB/s eta 0:00:02\n",
            "   -------------------------------- ------ 179.0/212.5 MB 33.6 MB/s eta 0:00:01\n",
            "   --------------------------------- ----- 185.1/212.5 MB 33.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- --- 191.9/212.5 MB 33.5 MB/s eta 0:00:01\n",
            "   ------------------------------------ -- 199.0/212.5 MB 33.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 203.7/212.5 MB 33.2 MB/s eta 0:00:01\n",
            "   --------------------------------------  210.0/212.5 MB 33.1 MB/s eta 0:00:01\n",
            "   --------------------------------------  212.3/212.5 MB 33.1 MB/s eta 0:00:01\n",
            "   --------------------------------------- 212.5/212.5 MB 31.5 MB/s eta 0:00:00\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Downloading setuptools-80.7.1-py3-none-any.whl (1.2 MB)\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.2/1.2 MB 13.3 MB/s eta 0:00:00\n",
            "Downloading MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
            "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 setuptools-80.7.1 sympy-1.14.0 torch-2.7.0 typing-extensions-4.13.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script isympy.exe is installed in 'c:\\Users\\saiki\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\saiki\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_6tvV3t1G6Z5",
        "outputId": "f41ada85-053c-4a9c-e7aa-219a385ad5cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\saiki\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
            "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# check the pytorch version\u001b[39;00m\n\u001b[32m      5\u001b[39m torch.__version__\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "# check the pytorch version\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPauPw-jIKZ3"
      },
      "source": [
        "## 1.Data (preparing and loading)\n",
        "Data can be almost anything... in machine learning.\n",
        "\n",
        "* Excel speadsheet\n",
        "* Images of any kind\n",
        "* Videos (YouTube has lots of data...)\n",
        "* Audio like songs or podcasts\n",
        "* DNA\n",
        "* Text\n",
        "\n",
        "Machine learning is a game of two parts:\n",
        "\n",
        "1.   Get data into a numerical representation\n",
        "2.   Build a model to learn patterns in that numerical representation.\n",
        "To showcase this, let's create some known data using the linear regression formula.\n",
        "\n",
        "\n",
        "We'll use a linear regression formula to make a straight line with known **parameters**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YEFnMMjHfrv",
        "outputId": "2cf6acc0-44f3-45a1-8230-090c28d14485"
      },
      "outputs": [],
      "source": [
        "# create known parameters\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "# create\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "X  = torch.arange(start,end,step).unsqueeze(dim=1)\n",
        "y = weight*X + bias\n",
        "X[:10],y[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et49vVopJsyd",
        "outputId": "0219bda2-5148-4736-f423-d594343da18b"
      },
      "outputs": [],
      "source": [
        "len(X),len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNFVeYDWKcSd",
        "outputId": "832995a5-61c7-48db-e5f3-eb2cba7c19de"
      },
      "outputs": [],
      "source": [
        "# create a train/test split\n",
        "train_split = int(0.8*len(X))\n",
        "X_train,y_train = X[:train_split],y[:train_split]\n",
        "X_test,y_test = X[train_split:],y[train_split:]\n",
        "len(X_train),len(y_train),len(X_test),len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Zs1G-_NK_b2"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(train_data = X_train,\n",
        "                     train_labels=y_train,\n",
        "                     test_data = X_test,\n",
        "                     test_labels = y_test,\n",
        "                     predictions=None):\n",
        "  plt.figure(figsize=(10,7))\n",
        "\n",
        "  plt.scatter(train_data,train_labels,c=\"r\",label=\"Train Data\")\n",
        "  plt.scatter(test_data,test_labels,c=\"g\",label=\"Test Data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    plt.scatter(test_data,predictions,c=\"b\",label=\"Predictions\")\n",
        "  plt.legend(prop={\"size\":14});\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "qzpwtxTpM_O-",
        "outputId": "020f05d0-150d-46e9-ab85-7ee1983e864c"
      },
      "outputs": [],
      "source": [
        "plot_predictions();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n8HVxKMNIsP"
      },
      "outputs": [],
      "source": [
        "## 2 Build the Model\n",
        "class LinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1,requires_grad=True,\n",
        "                                            dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1,\n",
        "                                         requires_grad=True,\n",
        "                                         dtype=torch.float\n",
        "                                         ))\n",
        "  def forward(self,x:torch.Tensor)-> torch.Tensor:\n",
        "    return self.weights*x+self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYIp8i6uRUxn"
      },
      "source": [
        "##PyTorch model building essentials\n",
        "* torch.nn - contains all of the buildings for computational graphs (a neural network can be considered a computational graph)\n",
        "* torch.nn.Parameter - what parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us\n",
        "* torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite forward()\n",
        "* torch.optim - this where the optimizers in PyTorch live, they will help with gradient descent\n",
        "* def forward() - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation\n",
        "\n",
        "extraInfo: https://pytorch.org/tutorials/beginner/ptcheat.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULjmKtPRRzej",
        "outputId": "e006a06a-4549-4892-9f6f-e69a2a304420"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model_0 = LinearRegressionModel()\n",
        "\n",
        "list(model_0.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-9usEYdRzcV",
        "outputId": "15a77d81-ee25-4b1b-e838-6c72a2ad09b7"
      },
      "outputs": [],
      "source": [
        "model_0.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7mlVytNRzZk",
        "outputId": "3beb0d39-0f06-452a-b40c-055c5ce73c4e"
      },
      "outputs": [],
      "source": [
        "weight,bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPaUW-iHT0yn"
      },
      "source": [
        "##Making prediction using `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvBs9s6vRzW_",
        "outputId": "fbeab968-ca36-4421-e455-9ad49a8b77e1"
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "  y_pred = model_0(X_test)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_preds = model_0(X_test)\n",
        "\n",
        "y_pred,y_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "RCbDFMc_UW92",
        "outputId": "718a8b87-54fd-45b2-daee-d19c8ceb21ed"
      },
      "outputs": [],
      "source": [
        "plot_predictions(predictions=y_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8efEFhwxazJj"
      },
      "source": [
        "## 3. Train model\n",
        "The whole idea of training is for a model to move from some unknown parameters (these may be random) to some known parameters.\n",
        "\n",
        "Or in other words from a poor representation of the data to a better representation of the data.\n",
        "\n",
        "One way to measure how poor or how wrong your models predictions are is to use a loss function.\n",
        "\n",
        "* Note: Loss function may also be called cost function or criterion in different areas. For our case, we're going to refer to it as a loss function.\n",
        "* Things we need to train:\n",
        "\n",
        "**Loss function:** A function to measure how wrong your model's predictions are to the ideal outputs, lower is better.\n",
        "\n",
        "**Optimizer:** Takes into account the loss of a model and adjusts the model's parameters (e.g. weight & bias in our case) to improve the loss function - https://pytorch.org/docs/stable/optim.html#module-torch.optim\n",
        "\n",
        "\n",
        "Inside the optimizer you'll often have to set two parameters:\n",
        "\n",
        "params - the model parameters you'd like to optimize, for example params=model_0.parameters()\n",
        "\n",
        "\n",
        "lr (learning rate) - the learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step (a small lr results in small changes, a large lr results in large changes)\n",
        "\n",
        "\n",
        "And specifically for PyTorch, we need:\n",
        "\n",
        "* A training loop\n",
        "* A testing loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjcPDsVJe6QQ",
        "outputId": "04dcb4ea-8cbc-4414-e07a-591ef75846a1"
      },
      "outputs": [],
      "source": [
        "# Check the model parameters\n",
        "model_0.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CLOt_v6RzUn"
      },
      "outputs": [],
      "source": [
        "# Setup a loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "# optimizer an optimizer\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP5st9kVRzRu"
      },
      "outputs": [],
      "source": [
        "# epochs = 100\n",
        "# ## training\n",
        "# for epoch in range(epochs):\n",
        "#   model_0.train()\n",
        "#   # 1.Forward pass\n",
        "#   y_pred = model_0(X_train)\n",
        "#   # 2.Calculate the loss\n",
        "#   loss = loss_fn(y_pred,y_train)\n",
        "#   print(f\"loss{loss}\")\n",
        "#   # 3.Optimizer zero grad\n",
        "#   optimizer.zero_grad()\n",
        "#   # 4.perform backpropagation on the loss\n",
        "#   loss.backward()\n",
        "#   # 5.step the optimizer\n",
        "#   optimizer.step()\n",
        "\n",
        "#   model_0.eval()\n",
        "#   print(model_0.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-4Eir8sXqgD"
      },
      "outputs": [],
      "source": [
        "# with torch.inference_mode():\n",
        "#   y_pred_new = model_0(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fHCAUa8XqdY"
      },
      "outputs": [],
      "source": [
        "# plot_predictions(predictions = y_pred_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN5NpuPsXqbA",
        "outputId": "bc887efe-03f2-4f44-fe2b-b97091fba25b"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# An epoch is one loop through the data... (this is a hyperparameter because we've set it ourselves)\n",
        "epochs = 200\n",
        "\n",
        "# Track different values\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "test_loss_values = []\n",
        "\n",
        "### Training\n",
        "# 0. Loop through the data\n",
        "for epoch in range(epochs):\n",
        "  # Set the model to training mode\n",
        "  model_0.train() # train mode in PyTorch sets all parameters that require gradients to require gradients\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_pred = model_0(X_train)\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Perform backpropagation on the loss with respect to the parameters of the model (calculate gradients of each parameter)\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Step the optimizer (perform gradient descent)\n",
        "  optimizer.step() # by default how the optimizer changes will accumulate through the loop so... we have to zero them above in step 3 for the next iteration of the loop\n",
        "\n",
        "  ### Testing\n",
        "  model_0.eval() # turns off different settings in the model not needed for evaluation/testing (dropout/batch norm layers)\n",
        "  with torch.inference_mode(): # turns off gradient tracking & a couple more things behind the scenes - https://twitter.com/PyTorch/status/1437838231505096708?s=20&t=aftDZicoiUGiklEP179x7A\n",
        "  # with torch.no_grad(): # you may also see torch.no_grad() in older PyTorch code\n",
        "    # 1. Do the forward pass\n",
        "    test_pred = model_0(X_test)\n",
        "\n",
        "    # 2. Calculate the loss\n",
        "    test_loss = loss_fn(test_pred, y_test)\n",
        "\n",
        "  # Print out what's happenin'\n",
        "  if epoch % 10 == 0:\n",
        "    epoch_count.append(epoch)\n",
        "    loss_values.append(loss)\n",
        "    test_loss_values.append(test_loss)\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")\n",
        "    # Print out model state_dict()\n",
        "    print(model_0.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RkR_aQGsMxW",
        "outputId": "13629ef3-c174-4e39-8e38-a9af5abc076a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.array(torch.tensor(loss_values).numpy()), test_loss_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "gzipR10Zs9VZ",
        "outputId": "60cc64ed-5d5b-4412-9ec4-c4cd43c9ca56"
      },
      "outputs": [],
      "source": [
        "# Plot the loss curves\n",
        "plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label=\"Train loss\")\n",
        "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkvq8UMPs-Lu",
        "outputId": "0d0798d1-6dc8-4784-d740-7167bf1cc172"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "# 1. Create models directory\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "# 2. Create model save path\n",
        "MODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME\n",
        "\n",
        "# 3. Save the model state dict\n",
        "print(f\"Saving model to :{MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_0.state_dict(),\n",
        "           f=MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buv4jyXSaB3t"
      },
      "outputs": [],
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouIGsUGhZCXC",
        "outputId": "8f22a42e-326c-4d77-e102-be23a1e450f8"
      },
      "outputs": [],
      "source": [
        "# Load a PyTorch model\n",
        "\n",
        "# Create a new instance of lienar regression model\n",
        "loaded_model_0 = LinearRegressionModel()\n",
        "\n",
        "# Load the saved model_1 state_dict\n",
        "loaded_model_0.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "# Put the loaded model to device\n",
        "loaded_model_0.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wEHwpCTZnke",
        "outputId": "07f0e19c-467b-48c3-89a1-2fc9a3f7b37a"
      },
      "outputs": [],
      "source": [
        "next(loaded_model_0.parameters()).device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4_17g6HaUuJ"
      },
      "outputs": [],
      "source": [
        "loaded_model_0.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1nyVEKhbK1I"
      },
      "outputs": [],
      "source": [
        "# Evaluate loaded model\n",
        "loaded_model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_model_0_preds = loaded_model_0(X_test)\n",
        "y_preds == loaded_model_0_preds"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "protein_pipeline",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
